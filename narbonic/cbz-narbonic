#!/usr/bin/env bash
#
# Run to automatically download all of Narbonic and generate CBZs of it.
# This downloads about 400MB of data from narbonic.com, and requires 4GB of
# disk space (23GB if you build the PNG version, and 26GB for both).
# The generated files are about 1.6GB (JPEG) or 13GB (PNG); the rest of the
# space is used for temporary files and can be reclaimed afterwards.

# CBZs are left in cbz.png/ for the PNG versions and cbz.jpeg/ for the JPEG
# versions. The PNG versions are ever so slightly higher quality but about
# 7x the size; it's usually not worth it, so they're disabled by default
# to speed up the build.

# Redesign notes
# Rather than doing format settings, we'll pack things into PDFs the way
# annotate-perfect-collection does it. This may even let us share some code.
# This means that our approach is now:
# - download each HTML page and extract just the comic container
# - resolve and download images, and update links in the HTML page
# - generate a no-commentary version by deleting the commentary divs
# - render HTML to PDF, crop PDF
# - at end of volume, concatenate PDFs together

BUILD_JPEG='yes'
BUILD_PNG='no'

# It also downloads the Narbonic music tracks and podcasts to music/.

DOWNLOAD_MUSIC='yes'

#### No user serviceable parts below this line. ####

set -e
shopt -s lastpipe
cd "$(dirname "$0")"
source ./narbonic.lib

function main {
  checkdeps htmldoc curl egrep sed wring pdfcrop identify
  mkdir -p tmp/{html,img} out
  # Download splash screen.
  [[ -f "img/chapter_splash.jpeg" ]] || {
    status "Downloading splash page..."
    >tmp/img/chapter_splash.jpeg curl --silent \
      "http://dlp2gfjvaz867.cloudfront.net/product_photos/2733272/narbonic1_big_400w.jpg"
  }
  # Generate the actual volumes.
  source narbonic.toc
  # Download music and podcasts.
  [[ $DOWNLOAD_MUSIC == yes ]] && download-music
}

#### Handlers for the index file. ####

CHAPTER_TEMPLATE='
<!-- CHAPTER %s -->
<!-- MEDIA SIZE 24x400cm -->
<div align="center">
  <img src="../img/chapter_splash.jpeg"><br>
  <h1>%s</h1>
</div>'

# State
title=""
outdir=""
default_commentary=off
commentary=off
page=0
skip_pages=no

# Mark the start of a volume.
# Mostly this just sets up metadata and makes sure the output directory exists.
function volume {
  [[ $title ]] && die "'volume $@' while still inside another volume ($title)!"
  status 'Volume: %s' "$1"
  title="$1"
  outdir="tmp/$1"
  commentary=$default_commentary
  page=0
  mkdir -p "$outdir"
}

# Mark the start of a chapter within a volume.
# This emits the start-of-chapter splash page.
function chapter {
  [[ $title ]] || die "'chapter $@' outside a volume!"
  commentary=$default_commentary
  if [[ "$1" == "Extras" && "$include_extras" == "no" ]]; then
    skip_pages=yes
    return
  else
    skip_pages=no
  fi
  ((++page))
  status '%s p.%03d - %s' "$title" "$page" "$1"
  local outfile="$(printf "%s/%03d CHAPTER %s" "$outdir" "$page" "$1")"
  printf "$CHAPTER_TEMPLATE" "$1" "$1" > "$outfile.html"
  html-to-pdf "$outfile.html" "$outfile.pdf"
}

# page <id> <title>
# write the next page out to the currently generated volume
# this includes fetching it if necessary, then extracting the necessary
# pieces of it.
function page {
  [[ $title ]] || die "'page $@' outside a volume!"
  [[ $skip_pages == yes ]] && return
  ((++page))
  fetch-page "$1" "$2"
  render-page "$1" "$2"
}

# Finalize a volume, rendering it into a PDF.
function endvolume {
  [[ $title ]] || die "'endvolume' outside a volume!"
  local out="out/${title}.pdf"
  status "Packing %s" "$out"
  # Finalize the previous volume.
  (
    cd "$outdir"
    pdfunite *.pdf ../../"$out"
  )
  pdfcrop --margins "20 50 20 50" "$out" "$out" >/dev/null
  status "Packed %s\n" "$out"
}

#### Downloading the HTML for each individual page. ####

# fetch-page <id>
# If page does not exist in the cache, downloads it.
# Then normalizes image paths and downloads all images not in cache.
function fetch-page {
  local name="$1"
  local title="$2"
  shift 2
  status "Fetch \"%s\" (%s)" "$name" "$title"
  local out="tmp/html/$name"
  if [[ ! -f $out ]]; then
    status "Fetch \"%s\" (%s) [download]" "$name" "$title"
    curl --retry 5 --silent "http://narbonic.com/comic/$name/" > "$out"
    chmod a-w "$out"
  else
    status "Fetch \"%s\" (%s) [cached]" "$name" "$title"
  fi
  fetch-images "$name"
}

# fetch-images <id>
# Fetch all images referenced in the given page that don't already exist locally.
function fetch-images {
  egrep -rho 'img src="[^"]+"' "tmp/html/$1" \
  | cut -d\" -f2 \
  | sed -E 's,^/,http://narbonic.com/,' \
  | while read link; do
      local tail="${link#*wp-content/uploads/}"
      if [[ -f "tmp/img/$tail" ]]; then
        status "Fetch $tail [cached]"
      else
        status "Fetch $tail [download]"
        mkdir -p "tmp/img/$(dirname "$tail")"
        curl --retry 5 --silent "$link" > "tmp/img/$tail"
      fi
  done
}

#### Converting the raw HTML into with and without-commentary versions ####

function exists? {
  while read file; do
    [[ -f $file ]] && echo "$file"
  done
}

function maxwidth-imgs {
  local html="$1"
  (
    cd "$(dirname "$html")"
    maxwidth $(egrep -o 'img src="[^"]+"' "$(basename "$html")" | cut -d'"' -f2 | exists?)
  )
}

# render-page <id> <title>
# Render the given page to tmp/$volume/$page
function render-page {
  local outfile="$(printf "%s/%03d %s %s" "$outdir" "$page" "$1" "$2")"
  status "Render $1"
  local infile="tmp/html/$1"
  echo "<!-- PAGE $1 -->" > "$outfile.html"
  echo "<!-- MEDIA SIZE 24x400cm -->" >> "$outfile.html"
  if [[ $commentary = on ]]; then
    wring html "$infile" .comic-area-wrapper
  else
    wring html "$infile" .comic-strip-image
  fi \
  | sed -E '
    s,img src=".*/wp-content/uploads/([^"]+)",img src="../img/\1",g
  ' >> "$outfile.html"
  html-to-pdf "$outfile.html" "$outfile.pdf"
}

function html-to-pdf {
  local width="$(maxwidth-imgs "$1")"
  htmldoc \
    --webpage --quiet \
    --footer . --linkcolor blue \
    --browserwidth "$width" \
    --fontsize 16 \
    -f "$2" "$1"
}

#### Music downloads ####

function download-music {
  echo "Downloading music..."
  mkdir -p music
  for track in 2009/08/MadbloodBattleAnthem.mp3 2011/08/Narbonic_Battle.mp3 2013/05/epilogue.mp3; do
    [[ -f music/"$(basename $track)" ]] && continue
    curl --silent -o music/"$(basename $track)" \
      http://narbonic.com/wordpress/wp-content/uploads/$track
  done
}

main "$@"

# Note: sometimes `wring` silently fails to download a page and emits a blank
# file. To fix this, run:
#   find html/orig/ -type f -name '*PAGE*' -print0 \
#   | xargs -0 fgrep -L 'comic-strip-container' \
#   | xargs -d\\n -r rm -v
# Which will delete the broken files and force the builder to redownload them.
# If you can run this with no output, there's nothing to fix.
# The script *should* detect and handle this automatically, but this command
# is included just in case.
