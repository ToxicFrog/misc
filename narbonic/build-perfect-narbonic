#!/usr/bin/env bash
set -e
shopt -s lastpipe
cd "$(basename "$0")"

### functions for toc files ###

# volume $name
# Set output name as given
VOLUME=""
function volume {
  VOLUME="$1"
  rm -rf "tmp/$VOLUME"
  mkdir -p "tmp/$VOLUME"
}

# Input settings.
SOURCE=""
function original { SOURCE="$1"; mkdir -p "tmp/$SOURCE"; }
DPI=150
function dpi { DPI="$1"; }

# stubbed out so that we can load the commentary index
function chapter {
  return 0
}

# page $name $title
# fetch the given page into the cache, if not already there
# then extract all commentary blocks from it and add them to the queue
CHEAD=0
function page {
  local name="$1"
  local title="$2"
  shift 2
  status "Commentary %04d \"%s\" (%s)" "$CHEAD" "$name" "$title"
  local out="tmp/html/$name"
  if [[ ! -f $out ]]; then
    status "Commentary %04d \"%s\" (%s) [download]" "$CHEAD" "$title" "$name"
    curl --retry 5 --silent "http://narbonic.com/comic/$name/" > "$out"
    chmod a-w "$out"
  else
    status "Commentary %04d \"%s\" (%s) [cached]" "$CHEAD" "$title" "$name"
  fi
  # at this point we have the page in cache, so extract commentary from it
  # unless we already have a *complete* commentary cache, in which case just
  # skip it
  [[ -f tmp/commentary/.complete ]] && return 0
  {
    # If we were passed extra arguments, extract those elements from the page,
    # in order. Otherwise, just scan the page for all "comic-strip-commentary-N"
    # blocks and extract those.
    if [[ $1 ]]; then printf '%s\n' "$@";
    else egrep -o 'comic-strip-commentary-[0-9]+' "$out" | sort -u
    fi
  } | while read id; do
    status "Commentary %04d \"%s\" (%s) [%s]" "$CHEAD" "$title" "$name" "$id"
    local div="$(wring html $out "#$id")"
    [[ ! $div ]] && continue
    local dst="tmp/commentary/$(printf %04d $CHEAD)"
    echo "$div" | sed -E 's,<br>,,g' > "$dst.html"
    ((++CHEAD))
    # fix up links in the comment fragment
    for link in $(egrep -o '/wordpress/wp-content/uploads/[^"]+' "$dst.html"); do
      status "Commentary %04d \"%s\" (%s) [fixup %s]" "$CHEAD" "$title" "$name" "$(basename $link)"
      local file="$dst-$(basename "$link")"
      [[ -f "$file" ]] || curl --retry 5 --silent "http://narbonic.com$link" > "$file"
      sed -E -i "s;$link;$(basename "$file");g" "$dst.html"
    done

  done
}

function next-commentary-id {
  local CTAIL=$(cat tmp/commentary/.tail)
  if (( CTAIL == CHEAD )); then
    die "queue underflow reading commentary at $CTAIL"
  fi
  printf "%04d" "$CTAIL"
  echo $((++CTAIL)) > tmp/commentary/.tail
}

function sync {
  local n="$1"
  case "$n" in
    -*|+*)
      local tail=$(cat tmp/commentary/.tail)
      tail=$((tail+n))
      ;;
    *)
      tail=$n
      ;;
  esac
  echo "$tail" > tmp/commentary/.tail
}

function css {
  echo "$1" > tmp/style.css
}

# Extract a single page in raster format from the PDF.
# Takes as argument the page number, 0-indexed.
# Emits the basename of the extracted page, with extension.
RASTER_MODE=pdfimages
function raster-page {
  local page="$(printf page-%03d $1)"
  case $RASTER_MODE in
    pdfimages)
      pdfimages -all -f $(($1+1)) -l $(($1+1)) "$SOURCE" tmp/raster
      mv tmp/raster-000.jpg "tmp/$SOURCE/$page.jpg"
      echo "$page.jpg"
      ;;
    imagemagick)
      convert \
        -define registry:temporary-path="$PWD/tmp/" \
        -density "$DPI" \
        "$SOURCE[$1]" \
        -colorspace gray \
        "tmp/$SOURCE/page-%03d.png"
      echo "$page.png"
      ;;
    imagemagick-aa)
      # sample at 2*DPI and then downscale
      convert \
        -define registry:temporary-path="$PWD/tmp/" \
        -density "$((DPI*2))" \
        "$SOURCE[$1]" \
        -colorspace gray \
        -scale '50%' \
        "tmp/$SOURCE/page-%03d.png"
      echo "$page.png"
      ;;
    *)
      die "unknown raster mode $RASTER_MODE"
      ;;
  esac
}

# copy $start [$end]
# copy the given pages, as is, from the input PDF to the output
function copy {
  local first="$1"
  if [[ $2 ]]; then
    local last="$2"
  else
    local last="$first"
  fi
  status "page %03d-%03d: copying" "$first" "$last"
  # The index file uses 0-indexing. pdfseparate uses 1-indexing. We standardize
  # on 0-indexing at the interfaces and adjust here.
  pdfseparate -f $((first+1)) -l $((last+1)) \
    "$SOURCE" "tmp/$VOLUME/page-%03d.pdf"
}

# whole $start [$end]
# copy the given pages, and for each one, pull commentary from the queue and
# insert it between the main page content and the page number.
function whole {
  local start=$1
  if [[ $2 ]]; then
    local end=$2; shift 2
  else
    local end=$start; shift 1
  fi
  for i in $(seq $start $end); do
    local page="$(raster-page $i)"
    status "$page: annotating"
    find-splits "tmp/$SOURCE/$page" "$@" | sort -g | tail -n1 \
    | add-commentary "tmp/$SOURCE/$page" "tmp/$VOLUME/$page"
  done
}

# split $start [$end]
# scan the given pages for distinct comic strips and split them apart; then, for
# each found strip, top to bottom, pull commentary from the queue and append it.
# finally, stitch the page back together and add it to the output.
function split {
  local start=$1
  if [[ $2 ]]; then
    local end=$2; shift 2
  else
    local end=$start; shift 1
  fi
  for i in $(seq $start $end); do
    local page="$(raster-page $i)"
    status "$page: splitting and annotating"
    find-splits "tmp/$SOURCE/$page" "$@" | add-commentary "tmp/$SOURCE/$page" "tmp/$VOLUME/$page"
  done
}

# split-at $page $split $split...
# split the given page (one only) at the given lines. Do not attempt to
# autodetect split points at all. Use only for pages where split detection
# fails badly.
function split-at {
  local page="$(raster-page $1)"
  shift
  status "$page: splitting and annotating"
  printf '%s\n' "$@" | add-commentary "tmp/$SOURCE/$page" "tmp/$VOLUME/$page"
}

# endvolume
# finalize the volume by packing it into a PDF or a CBZ
function endvolume {
  if [[ ! $VOLUME ]]; then die "endvolume without matching volume"; fi
  if [[ $skip ]]; then return 0; fi
  status "Packing $VOLUME.pdf"
  cd "tmp/$VOLUME"
  pdfunite *.pdf "../../out/$VOLUME.pdf"
  # convert *.jpg "../../out/$VOLUME.pdf"
  status "Packed out/$VOLUME.pdf\\n"
  # status "Packing $VOLUME.cbz"
  # zip --quiet -0 -r "../../out/$VOLUME.cbz" .
  # status "Packed out/$VOLUME.cbz\\n"
  cd ../..
  VOLUME=""
}

# supporting functions

# find-splits $img
# scan it for horizontal split points, output the Y coordinate of each split
# on stdout
# FIXME: this doesn't work if there's no page number, since the final split
# is never detected
function find-splits {
  local img="$1"; shift
  if [[ $1 ]]; then printf "%s\n" "$@"; fi
  convert "$img" -resize '1x!' txt: | lua -e '
    local sof,eof
    local didsplit = false
    local depth = 0
    for line in io.lines() do
      --local y,colour = line:match("^0,(%d+):.*gray%(255%)$")
      local y,colour = line:match("^0,(%d+):.*#FFFFFF")
      if y then
        depth = 0
        sof = sof or tonumber(y)
        eof = tonumber(y)
        --io.stderr:write("sof: "..y.."\n")
      elseif not y then
        depth = depth+1
        if sof and eof
          -- ugly heuristics
          and depth > 2
          and sof > 200
          and eof - sof >= 8 then
          --io.stderr:write("split: "..sof.."-"..eof.."\n")
          print(math.floor(sof + (eof-sof)/4))
          didsplit = true
        end
        if depth > 2 then
          sof,eof = nil
        end
      end
    end
    -- hack so that if we found NO splits, which usually means a single-page
    -- comic so large that it obscures the page number, we forcibly emit one
    -- at the very end
    if not didsplit and sof and eof then
      print(math.floor((sof+eof)/2))
    end'
}

function find-margins {
  local LM=$(convert "$1" -resize '!x1' txt: | egrep -v '^#' | fgrep -v 'gray(255)' | head -n1 | cut -d, -f1)
  local RM=$(convert "$1" -resize '!x1' txt: | egrep -v '^#' | fgrep -v 'gray(255)' | tail -n1 | cut -d, -f1)
  printf '%d %d\n' $LM $RM
}

function render-commentary {
  local cid="$1"
  local width="$2"
  local out="tmp/commentary/$cid-$(md5sum tmp/style.css | cut -c1-4)-$width.png"
  if [[ ( ! -f $out ) || ( $out -ot tmp/commentary/$cid.html ) ]]; then
    #status "Render commentary %s at width %s" $cid $width
    wkhtmltoimage --format png --enable-local-file-access --quiet \
      --user-style-sheet tmp/style.css --width $width \
      tmp/commentary/$cid.html "$out"
  fi
  echo "$out"
}

# add-commentary IN OUT
# add commentary to IN and write the resulting image to OUT
# takes on stdin the Y-coordinates at which to insert the commentary; pulls
# the commentary from the commentary queue
function add-commentary {
  local input="$1"
  local page="$(basename "$input")"
  local output="$2"
  local -a args=("$input" -background white)
  local LM RM width
  find-margins "$input" | read LM RM
  local width=$((RM-LM))

  declare -a lines=()
  sort -g | while read Y; do
    local cid=$(next-commentary-id)
    lines+=("$Y $cid")
  done
  # we don't just pipe one into the other so that next-commentary-id can update
  # CTAIL correctly
  printf '%s\n' "${lines[@]}" | tac | while read Y cid; do
    status "$page: render commentary %s @ (%d,%d)" $cid $LM $Y
    local commentary="$(render-commentary $cid $width)"
    if [[ $skip ]]; then continue; fi
    local H=$(identify "$commentary" | cut -d' ' -f3 | cut -dx -f2)
    args+=(-splice 0x$H+0+$Y -page +$LM+$Y "$commentary" -flatten)
  done
  # status '%s' "convert ${args[*]} $output"
  if [[ $skip ]]; then
    convert "$input" raw:/dev/null
  else
    status "$page: writing $output"
    convert "${args[@]}" -colorspace gray "$output"
    convert "$output" "${output%.*}.pdf"
  fi
}

# to insert commentary
# given an image and a sequence of (Y, commentary) pairs:
# - get the margins LM and RM for the image
# - for each pair
#   - render the commentary to C with width (RM-LM)
#   - get the height H of C
#   - use "-background white -splice 0xH+0+Y" to insert the necessary whitespace
#   - use "-page +RM+Y C" to insert the commentary overtop of it
#   - finally write out with "-flatten out.jpeg"
# ideally we want to do all of these steps in one "convert" call so that we don't
# end up repeatedly decompressing and recompressing the image
# we need to do the splices and inserts from the bottom up because otherwise
# each one will affect the coordinates of the ones below it

# Now the actual machinery of the program

set -e
shopt -s lastpipe

function main {
  checkdeps \
    wkhtmltoimage \
    convert identify \
    pdfimages pdfseparate pdfunite \
    wring \
    gs \
    curl zip fgrep egrep cut sed
  mkdir -p tmp/{commentary,html} out
  # Load commentary
  source narbonic.perfect-commentary
  echo 0 > tmp/commentary/.tail
  if [[ -f tmp/commentary/.complete ]]; then
    CHEAD=$(ls tmp/commentary/*.html | wc -l)
    ((++CHEAD))
  else
    touch tmp/commentary/.complete
  fi
  status "Done initializing commentary with $CHEAD entries; building images next.\\n"
  # Generate the actual volumes.
  source narbonic.perfect-index
}

function status {
  local fmt="$1"; shift
  printf '\r\x1B[2K'"$fmt" "$@" >&2
}

if [[ $DEBUG ]]; then
  function status {
    local fmt="$1"; shift
    printf "$fmt\n" "$@" >&2
  }
fi

function checkdeps {
  local missing=""
  for command in "$@"; do
    if ! type "$command" 2>/dev/null >/dev/null; then
      missing+=" $command"
    fi
  done
  if [[ "$missing" ]]; then
    echo "You are missing the following commands:$missing" >&2
    echo "Please install them and try again." >&2
    exit 1
  fi
}

function die {
  echo "$@" >&2
  exit 1
}

main
