#!/usr/bin/env bash
set -e
shopt -s lastpipe

# WIP on a tool that works similar to build-narbonic, but extracts the images
# from the Narbonic: The Perfect Collection PDFs and adds commentary to them
# before repacking them into CBZ (or maybe back into PDF? idk)
# this is made more difficult by the fact that the PDF is one page per image,
# but each page typically contains either three daily strips or one sunday
# strip, while the website is typically six daily strips or one sunday strip
# per page.
# So, we need to split each page in the PDF up into subpages, then match each
# subpage up with the corresponding commentary.
# We can get commentary from the HTML cache; each HTML file contains commentary
# blocks annotated as class="comic-strip-commentary" id="comic-strip-commentary-N",
# where N typically runs from 1 to 6.

# To split a PDF image, we can do something like:
# convert page.jpeg -resize '1x!' txt:
# this outputs one line per row, and rows that were entirely whitespace are
# written as:
# 0,0: (255,255,255)  #FFFFFF  gray(255)
# while non-whitespace rows will be (e.g.)
# 0,87: (254.969,254.969,254.969)  #FFFFFF  gray(99.9878%)
# leading and trailing runs of whitespace will correspond to the page top and
# bottom margins
# there will be a small blob of non-whitespace at the bottom (~25px high or so)
# which is the page number, and should be left at the bottom of the page
# apart from that we can probably assume that a run of whitespace more than 5px
# high divides comic strips on the page

# So, our input is a collection of images emitted by `pdfimages` from the two
# Perfect Collection volumes
# For each image, we then need to either:
# - drop it
# - include it as is in the output
# - include it as a single image, but append commentary
#   - look up commentary for that page
#   - split the page between the page number and the bottom of the comic,
#     and insert the rendered commentary
# - include it as multiple images
#   - find the inter-image divisions
#   - split each image into a separate file, probably like:
#       margin+image - middle image - bottom image - page number+margin
#   - find the commentary for each image and insert it immediately after the image
#   - weave the images back together into a single page
# So we need an input page control map that says what to do with what pages

# For volume 1 this might start out looking something like
# copy 0-10
# split 11 12
# annotate 13
# split 14-53
# ...etc
# Ideally, it would be smart enough to mostly-automatically map PDF page/subpage
# IDs to commentary IDs perhaps with occasional manual resyncs, but this seems
# unlikely. We'll need some way to map strip IDs to commentary IDs in the cache,
# identified by HTML page and then by commentary ID within the page.

# It's tempting to do this "commentary first", where we start with the pages in
# the cache containing image references and commentary, and then have a mapping
# that lets us rewrite the image references to images extracted from the Perfect
# Collection. That might be easier overall, and it would preserve all the sunday
# strips, etc -- I might keep that as an alternate operating mode, come to think
# of it. But it will also result in a lot of visual inconsistency since we'll be
# mixing Perfect Collection strips with web strips, and I'd like to keep the
# ordering and layout of the Perfect Collection as an option as the "most distilled"
# way to read Narbonic.

# If we were laying this out for print, I might do comics on one page and
# commentary on the facing pages, but since this is going to be for digital reading,
# I think the same layout as the website makes sense -- comic with commentary
# underneath.

# Ok, our basic operations here are:

# volume $name
# Set output name as given
VOLUME=""
function volume {
  VOLUME="$1"
  rm -rf "tmp/$VOLUME"
  mkdir -p "tmp/$VOLUME"
}

# images-from $pdf
# extract images from the given PDF file
SOURCE=""
function images-from {
  status "Extracting images from %s..." "$1"
  SOURCE="$(basename "$1")"
  mkdir -p "tmp/$SOURCE"
  pdfimages -all "$1" "tmp/$SOURCE/page"
}

DPI=150
function dpi {
  DPI="$1"
}

function images-from-2 {
  status "Extracting images from %s..." "$1"
  SOURCE="$(basename "$1")"
  mkdir -p "tmp/$SOURCE"
  shift
  return
  local pages=$(identify "$SOURCE" | wc -l)
  for page in $(seq 0 20 $pages); do
    status "Extracting images from %s [pages $page-$((page+19))]..." "$1"
    convert \
      -define registry:temporary-path="$PWD/tmp/" \
      -density "$DPI" \
      "$SOURCE[$page-$((page+19))]" \
      -quality 100 \
      -colorspace gray \
      "tmp/$SOURCE/page-%03d.jpg"
  done
  # redo the cover in full colour
  convert \
    -define registry:temporary-path="$PWD/tmp/" \
    -density 300 \
    "$SOURCE[0]" \
    -quality 100 \
    "tmp/$SOURCE/page-%03d.jpg"
}

# stubbed out so that we can load the index
function chapter {
  return 0
}

# page $name $title
# fetch the given page into the cache, if not already there
# then extract all commentary blocks from it and add them to the queue
CHEAD=0
function page {
  local name="$1"
  local title="$2"
  shift 2
  status "Commentary for \"%s\" (%s)" "$name" "$title"
  local out="tmp/html/$name"
  if [[ ! -f $out ]]; then
    status "Commentary for \"%s\" (%s) [download]" "$title" "$name"
    curl --retry 5 --silent "http://narbonic.com/comic/$name/" > "$out"
  else
    status "Commentary for \"%s\" (%s) [cached]" "$title" "$name"
  fi
  # at this point we have the page in cache, so extract commentary from it
  # unless we already have a *complete* commentary cache, in which case just
  # skip it
  [[ -f tmp/commentary/.complete ]] && return 0
  {
    # If we were passed extra arguments, extract those elements from the page,
    # in order. Otherwise, just scan the page for all "comic-strip-commentary-N"
    # blocks and extract those.
    if [[ $1 ]]; then printf '%s\n' "$@";
    else egrep -o 'comic-strip-commentary-[0-9]+' "$out" | sort -u
    fi
  } | while read id; do
    status "Commentary for \"%s\" (%s) [$id]" "$title" "$name"
    local div="$(wring html $out "#$id")"
    [[ ! $div ]] && continue
    echo "$div" | sed -E 's,<br>,,g' > "tmp/commentary/$(printf %04d $CHEAD).html"
    ((++CHEAD))
  done
}

function next-commentary-id {
  local CTAIL=$(cat tmp/commentary/.tail)
  if (( CTAIL == CHEAD )); then
    die "queue underflow reading commentary at $CTAIL"
  fi
  printf "%04d" "$CTAIL"
  echo $((++CTAIL)) > tmp/commentary/.tail
}

function sync {
  echo "$1" > tmp/commentary/.tail
}

function css {
  echo "$1" > tmp/style.css
}

# copy $start [$end]
# copy the given pages, as is, from the input PDF to the output
function copy {
  local start=$1
  local end=$2
  [[ $end ]] || end=$start
  for i in $(seq $start $end); do
    local page="$(printf page-%03d.jpg $i)"
    status "$page: copying"
    ln "tmp/$SOURCE/$page" "tmp/$VOLUME/$page"
  done
}

# whole $start [$end]
# copy the given pages, and for each one, pull commentary from the queue and
# insert it between the main page content and the page number.
function whole {
  local start=$1
  if [[ $2 ]]; then
    local end=$2; shift 2
  else
    local end=$start; shift 1
  fi
  for i in $(seq $start $end); do
    local page="$(printf page-%03d.jpg $i)"
    status "$page: annotating"
    find-splits "tmp/$SOURCE/$page" "$@" | sort -g | tail -n1 | add-commentary "tmp/$SOURCE/$page" "tmp/$VOLUME/$page"
  done
}

# split $start [$end]
# scan the given pages for distinct comic strips and split them apart; then, for
# each found strip, top to bottom, pull commentary from the queue and append it.
# finally, stitch the page back together and add it to the output.
function split {
  local start=$1
  if [[ $2 ]]; then
    local end=$2; shift 2
  else
    local end=$start; shift 1
  fi
  for i in $(seq $start $end); do
    local page="$(printf page-%03d.jpg $i)"
    status "$page: splitting and annotating"
    find-splits "tmp/$SOURCE/$page" "$@" | add-commentary "tmp/$SOURCE/$page" "tmp/$VOLUME/$page"
  done
}

# split-at $page $split $split...
# split the given page (one only) at the given lines. Do not attempt to
# autodetect split points at all. Use only for pages where split detection
# fails badly.
function split-at {
  local page="$(printf page-%03d.jpg $1)"
  shift
  status "$page: splitting and annotating"
  printf '%s\n' "$@" | add-commentary "tmp/$SOURCE/$page" "tmp/$VOLUME/$page"
}

# endvolume
# finalize the volume by packing it into a PDF or a CBZ
function endvolume {
  if [[ ! $VOLUME ]]; then die "endvolume without matching volume"; fi
  if [[ $skip ]]; then return 0; fi
  status "Packing $VOLUME.pdf"
  cd "tmp/$VOLUME"
  convert *.jpg "../../out/$VOLUME.pdf"
  status "Packed out/$VOLUME.pdf\\n"
  status "Packing $VOLUME.cbz"
  zip --quiet -0 -r "../../out/$VOLUME.cbz" .
  status "Packed out/$VOLUME.cbz\\n"
  cd ../..
  VOLUME=""
}

# supporting functions

# find-splits $img
# scan it for horizontal split points, output the Y coordinate of each split
# on stdout
# FIXME: this doesn't work if there's no page number, since the final split
# is never detected
function find-splits {
  local img="$1"; shift
  if [[ $1 ]]; then printf "%s\n" "$@"; fi
  convert "$img" -resize '1x!' txt: | lua -e '
    local sof,eof
    local didsplit = false
    local depth = 0
    for line in io.lines() do
      --local y,colour = line:match("^0,(%d+):.*gray%(255%)$")
      local y,colour = line:match("^0,(%d+):.*#FFFFFF")
      if y then
        depth = 0
        sof = sof or tonumber(y)
        eof = tonumber(y)
        --io.stderr:write("sof: "..y.."\n")
      elseif not y then
        depth = depth+1
        if sof and eof
          -- ugly heuristics
          and depth > 2
          and sof > 200
          and eof - sof >= 8 then
          --io.stderr:write("split: "..sof.."-"..eof.."\n")
          print(math.floor(sof + (eof-sof)/4))
          didsplit = true
        end
        if depth > 2 then
          sof,eof = nil
        end
      end
    end
    -- hack so that if we found NO splits, which usually means a single-page
    -- comic so large that it obscures the page number, we forcibly emit one
    -- at the very end
    if not didsplit and sof and eof then
      print(math.floor((sof+eof)/2))
    end'
}

function find-margins {
  local LM=$(convert "$1" -resize '!x1' txt: | egrep -v '^#' | fgrep -v 'gray(255)' | head -n1 | cut -d, -f1)
  local RM=$(convert "$1" -resize '!x1' txt: | egrep -v '^#' | fgrep -v 'gray(255)' | tail -n1 | cut -d, -f1)
  printf '%d %d\n' $LM $RM
}

# add-commentary IN OUT
# add commentary to IN and write the resulting image to OUT
# takes on stdin the Y-coordinates at which to insert the commentary; pulls
# the commentary from the commentary queue
function add-commentary {
  local input="$1"
  local page="$(basename "$input")"
  local output="$2"
  local -a args=("$input" -background white)
  local LM RM width
  find-margins "$input" | read LM RM
  width=$((RM-LM))

  declare -a lines=()
  sort -g | while read Y; do
    local cid=$(next-commentary-id)
    lines+=("$Y $cid")
  done
  # we don't just pipe one into the other so that next-commentary-id can update
  # CTAIL correctly
  printf '%s\n' "${lines[@]}" | tac | while read Y cid; do
    status "$page: render commentary %s @ (%d,%d)" $cid $LM $Y
    if [[ $skip ]]; then continue; fi
    wkhtmltoimage --format png --enable-local-file-access --quiet \
      --user-style-sheet tmp/style.css --width $width \
      tmp/commentary/$cid.html tmp/commentary/$cid.png
    local H=$(identify tmp/commentary/$cid.png | cut -d' ' -f3 | cut -dx -f2)
    args+=(-splice 0x$H+0+$Y -page +$LM+$Y tmp/commentary/$cid.png -flatten)
  done
  # status '%s' "convert ${args[*]} $output"
  if [[ $skip ]]; then
    convert "$input" raw:/dev/null
  else
    status "$page: raveling strips and commentary to $output"
    convert "${args[@]}" -quality 95 "$output"
  fi
}

# to insert commentary
# given an image and a sequence of (Y, commentary) pairs:
# - get the margins LM and RM for the image
# - for each pair
#   - render the commentary to C with width (RM-LM)
#   - get the height H of C
#   - use "-background white -splice 0xH+0+Y" to insert the necessary whitespace
#   - use "-page +RM+Y C" to insert the commentary overtop of it
#   - finally write out with "-flatten out.jpeg"
# ideally we want to do all of these steps in one "convert" call so that we don't
# end up repeatedly decompressing and recompressing the image
# we need to do the splices and inserts from the bottom up because otherwise
# each one will affect the coordinates of the ones below it

# Now the actual machinery of the program

set -e
shopt -s lastpipe

function main {
  checkdeps wkhtmltoimage curl wring zip pdfimages convert fgrep cut gs
  mkdir -p tmp/{commentary,html} out
  # Load commentary
  source narbonic.perfect-commentary
  echo 0 > tmp/commentary/.tail
  if [[ -f tmp/commentary/.complete ]]; then
    CHEAD=$(ls tmp/commentary | wc -l)
    ((++CHEAD))
  else
    touch tmp/commentary/.complete
  fi
  status "Done initializing commentary; building images next.\\n"
  # Generate the actual volumes.
  source narbonic.perfect-index
}

function status {
  local fmt="$1"; shift
  printf '\n\x1B[2K'"$fmt" "$@"
}

function checkdeps {
  local missing=""
  for command in "$@"; do
    if ! type "$command" 2>/dev/null >/dev/null; then
      missing+=" $command"
    fi
  done
  if [[ "$missing" ]]; then
    echo "You are missing the following commands:$missing" >&2
    echo "Please install them and try again." >&2
    exit 1
  fi
}

function die {
  echo "$@" >&2
  exit 1
}

main
